import streamlit as st
import os
import tiktoken
import streamlit as st

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

#models
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

from dotenv import load_dotenv


# Streamlit Cloudã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰OPENAI_API_KEYã‚’å–å¾—
openai_api_key = st.secrets["openai"]["OPENAI_API_KEY"]

# ChatOpenAI ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
model_instance = ChatOpenAI(
    temperature=temperature, 
    model_name=model_name, 
    openai_api_key=openai_api_key  # APIã‚­ãƒ¼ã‚’æ¸¡ã™
)

if not openai_api_key:
    st.error("ğŸ”‘ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

MODEL_PRICES = {
    "input": {
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-4o": 5 / 1_000_000,
        "claude-3-5-sonnet-20240620": 3 / 1_000_000,
        "gemini-1.5-pro-latest": 3.5 / 1_000_000
    },
    "output": {
        "gpt-3.5-turbo": 1.5 / 1_000_000,
        "gpt-4o": 15 / 1_000_000,
        "claude-3-5-sonnet-20240620": 15 / 1_000_000,
        "gemini-1.5-pro-latest": 10.5 / 1_000_000
    }
}

def init_sidebar():
    st.sidebar.title("")

    # ğŸ”¹ **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã®ãƒšãƒ¼ã‚¸ã‚’é¸æŠ**
    page = st.sidebar.selectbox("ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠ:", ["Home", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3"])
    # ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.sidebar.button("ğŸ—‘ Clear Conversation"):
        st.session_state.message_history = [("system", "ALTAM is a helpful assistant.")]

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    models = ("GPT-3.5", "GPT-4", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    model = st.sidebar.radio("ğŸ¤– Choose a model:", models)

    # Temperature ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼
    temperature = st.sidebar.slider("Temperature:", 0.0, 2.0, 0.0, 0.01)
  
    model_instance = None  # åˆæœŸå€¤
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo"
        model_instance = ChatOpenAI(temperature=temperature, model_name=st.session_state.model_name)
    elif model == "GPT-4":
        st.session_state.model_name = "gpt-4o"
        model_instance = ChatOpenAI(temperature=temperature, model_name=st.session_state.model_name)
    elif model == "Claude 3.5 Sonnet":
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        model_instance = ChatAnthropic(temperature=temperature, model_name=st.session_state.model_name)
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        model_instance = ChatGoogleGenerativeAI(temperature=temperature, model=st.session_state.model_name)
    return model_instance, page  # å¿…ãš page ã‚‚è¿”ã™


def load_project_data(project_name):
    #"""ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
    #    project_files = {
    #"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1": "data/financial_report.pdf",
    #"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2": "data/medical_paper.csv",
    #"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3": "data/python_tutorial.md"
    #}
    # # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    #data_file = project_files.get(project_name, None)
     # `st.session_state` ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ or ãƒ‡ãƒ¼ã‚¿å†…å®¹ï¼‰
    #if data_file:
        #with open(data_file, "r", encoding="utf-8") as f:
    #        st.session_state.project_data = f.read()
    #else:
    #    st.session_state.project_data = "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ç‰¹å®šã®è³‡æ–™ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    #return data_file

    if project_name == "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1":
        return "ãƒ‡ãƒ¼ã‚¿1: é‡‘èãƒ¬ãƒãƒ¼ãƒˆ"
    elif project_name == "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2":
        return "ãƒ‡ãƒ¼ã‚¿2: åŒ»ç™‚è«–æ–‡"
    elif project_name == "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3":
        return "ãƒ‡ãƒ¼ã‚¿3: Pythonãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"
    return "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿"


def main():

    st.set_page_config(
        page_title="I am ALTAMGPT",
        page_icon="ğŸ¤–"
    )

     #ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆãƒ¢ãƒ‡ãƒ« + ãƒšãƒ¼ã‚¸é¸æŠï¼‰
    st.session_state.llm, page = init_sidebar()
 
    
      #ãƒšãƒ¼ã‚¸ã”ã¨ã®è¡¨ç¤ºå‡¦ç†
    if page == "Home":
        st.header("ğŸ  Home - ALTAM SOFTWARE OF LLM")
        st.write("Welcome to ALTAMGPT! ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return
    
        #ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    project_data = load_project_data(page)
        #ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ˜ç¤º
    st.write(f"ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ **{project_data}** ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
  
     # CSSã‚’é©ç”¨ï¼ˆèƒŒæ™¯è‰²å¤‰æ›´ & ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ‡ã‚¶ã‚¤ãƒ³ï¼‰**
    st.markdown(
        """
    <style>
    /* ãƒšãƒ¼ã‚¸å…¨ä½“ã®èƒŒæ™¯ */
    body {
        background-color: #e0f7fa; /* è–„ã„æ°´è‰² */
    }

    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®èƒŒæ™¯ã‚’é»’ã«å¤‰æ›´ */
    [data-testid="stSidebar"] {
        background-color: #1E1E1E !important;  /* ãƒ€ãƒ¼ã‚¯ã‚°ãƒ¬ãƒ¼ï¼ˆé»’ã«è¿‘ã„ï¼‰ */
        color: white !important;  /* æ–‡å­—è‰²ã‚’ç™½ã« */
    }

    /*ã‚µã‚¤ãƒ‰ãƒãƒ¼å†…ã®ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç™½ã«å¤‰æ›´ã™ã‚‹*/
    section[data-testid="stSidebar"] button {
        background-color: #333 !important;  /* ãƒœã‚¿ãƒ³ã‚’ã‚°ãƒ¬ãƒ¼ */
        color: white !important;  /* ãƒœã‚¿ãƒ³ã®æ–‡å­—è‰²ã‚’ç™½ */
        border: 1px solid #555 !important;  /* æ ç·šã‚’æš—ã‚ã®ã‚°ãƒ¬ãƒ¼ */
    }

    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼å†…ã®ãƒœã‚¿ãƒ³ã®ãƒ‡ã‚¶ã‚¤ãƒ³å¤‰æ›´ */
    [data-testid="stSidebar"] button {
        background-color: #333 !important;  /* ãƒœã‚¿ãƒ³ã‚’ã‚°ãƒ¬ãƒ¼ */
        color: white !important;  /* ãƒœã‚¿ãƒ³ã®æ–‡å­—è‰²ã‚’ç™½ */
        border: 1px solid #555 !important;  /* æ ç·šã‚’æš—ã‚ã®ã‚°ãƒ¬ãƒ¼ */
    }

    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ï¼ˆTemperatureï¼‰ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º */
    [data-testid="stSidebar"] .stSlider {
        color: white !important;
    }

    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠï¼‰ */
    [data-testid="stSidebar"] label {
        color: white !important; /* ãƒ©ãƒ™ãƒ«ã®æ–‡å­—è‰²ã‚’ç™½ */
    }
    </style>
    """,
    unsafe_allow_html=True
)


    # ãƒšãƒ¼ã‚¸ã”ã¨ã®è¡¨ç¤ºå‡¦ç†**
    if page == "Home":
        st.header("ğŸ  Home - ALTAM SOFTWARE OF LLM")
        st.write("Welcome to ALTAMGPT! ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return  # Homeã§ã¯ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ãªã—

     # Home ä»¥å¤–ã®ãƒšãƒ¼ã‚¸ã§ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’è¡¨ç¤º**
    st.header(f"ğŸ“‚ {page}")
    chat_interface()

 


def chat_interface():

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–: message_history ãŒãªã‘ã‚Œã°ä½œæˆ
    if "message_history" not in st.session_state:
        st.session_state["message_history"] = [
            # System Prompt ã‚’è¨­å®š ('system' ã¯System Promptã‚’æ„å‘³ã™ã‚‹)
            ("system", "ALTAM is a helpful assistant.")
        ]

    # ChatGPTã«è³ªå•ã‚’ä¸ãˆã¦å›ç­”ã‚’å–ã‚Šå‡ºã™(ãƒ‘ãƒ¼ã‚¹ã™ã‚‹)å‡¦ç†ã‚’ä½œæˆ (1.-4.ã®å‡¦ç†)
    # 1. ChatGPTã®ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™ã‚ˆã†ã«è¨­å®š
    #    (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯GPT-3.5 TurboãŒå‘¼ã°ã‚Œã‚‹)
    llm = st.session_state.llm


    # 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å—ã‘å–ã‚Šã€ChatGPTã«æ¸¡ã™ãŸã‚ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    #    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã¯éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å«ã‚ã‚‹ã‚ˆã†ã«è¨­å®š

    N = 3 #ç›´è¿‘3ã¤ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã ã‘ã‚’APIã«é€ã‚‹ã€‚

    prompt = ChatPromptTemplate.from_messages([
        *st.session_state["message_history"][-N:],#æœ€æ–°ã®ã‚‚ã®ã®ã¿è¡¨ç¤ºã™ã‚‹ã€‚
        ("user", "{user_input}")  # ã“ã“ã«ã‚ã¨ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒå…¥ã‚‹
    ])


    # 3. ChatGPTã®è¿”ç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ãŸã‚ã®å‡¦ç†ã‚’å‘¼ã³å‡ºã—
    output_parser = StrOutputParser()


    # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ChatGPTã«æ¸¡ã—ã€è¿”ç­”ã‚’å–ã‚Šå‡ºã™é€£ç¶šçš„ãªå‡¦ç†(chain)ã‚’ä½œæˆ
    #    å„è¦ç´ ã‚’ | (ãƒ‘ã‚¤ãƒ—) ã§ã¤ãªã’ã¦é€£ç¶šçš„ãªå‡¦ç†ã‚’ä½œæˆã™ã‚‹ã®ãŒLCELã®ç‰¹å¾´
    chain = prompt | llm | output_parser

     #å…¥åŠ›ã®éƒ¨åˆ†ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚
    container = st.container()
    with container:
        with st.form(key="myform", clear_on_submit=True):
            user_input = st.text_area(label="Message:", key="input", height=100)
            submit_button = st.form_submit_button(label = "Send")

            if submit_button and user_input:
                #å…¥åŠ›ã•ã‚Œã¦ãƒœã‚¿ãƒ³ã‚’æŠ¼ã•ã‚ŒãŸã‚‰å®Ÿè¡Œ

                with st.spinner("ChatGPT is typing ..."):
                    response = chain.invoke({"user_input": user_input})

                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ  ('user' ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’æ„å‘³ã™ã‚‹)
                st.session_state.message_history.append(("user", user_input))

            # ChatGPTã®å›ç­”ã‚’å±¥æ­´ã«è¿½åŠ  ('assistant' ã¯ChatGPTã®å›ç­”ã‚’æ„å‘³ã™ã‚‹)
                st.session_state.message_history.append(("ai", response))

      # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿è¡¨ç¤ºï¼ˆrerun ãªã—ã§å³æ™‚åæ˜ ï¼‰
        if "last_message" in st.session_state:
           with st.chat_message("assistant"):
                st.markdown(st.session_state["last_message"])

    with st.spinner("ChatGPT is trying"):
        response = chain.invoke({"user_input": user_input})

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        with st.chat_message(role):
            st.markdown(message)


if __name__ == '__main__':
    main()
